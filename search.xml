<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[变分自编码机(VAE)]]></title>
    <url>%2F2018%2F03%2F13%2FVAE%2F</url>
    <content type="text"><![CDATA[变分自编码机(VAE) 从EM到变分推断 我们假设有一个隐变量z，我们的样本\(x^{( i)}\)是从\(p_{\theta }( x|z)\)中产生，因为有隐变量的存在，通常\(p_{\theta }( x) =\int p_{\theta }( z) p_{\theta }( x|z) dz\)的边缘分布是没法算的。 所以传统来说，我们会构造出一个下界： \[ \begin{aligned} \log p( x) &amp; =\underbrace{E_{z\sim q( z)}(\log p( x ,z)) -H( q)}_{ELOB} +KL( q( z) ||p( z |x)) \end{aligned} \] 而EM算法，就是通过精心选择这个下界中的q，从而使得下界最大化，也就是计算\(q( z) =p( z|x)\)来近似该模型的似然度。进一步可以参考我之前写的文章《带你理解EM算法》 然而如果我们令\(q( z) =p( z|x) =\frac{p_{\theta }( x|z) p_{\theta }( z)}{p_{\theta }( x)}\)也是不可计算的呢，比如你的z有很多很多维，那么你在算那个期望的时候就会出现一堆积分，这是非常难算的。 此时我们可以使用变分推断的方法，那就是，我们不直接令\(q( z) =p( z|x)\)了，而是选一个相对简单的分布q去近似\(p( z|x)\)。简单的q怎么来？最常用的就是对q作平均场(mean-field)假设，即，我们可以认为： \[ q(\mathbf{z}) =\prod _{i} q_{i}( z_{i}) \] 这个假设的意思是，虽然你的z有很多维，但是他们都是相互独立的，也就是说，你算很多很多积分的时候，每个\(z_{i}\)可以分别积分，所以一个联合积分的问题就简化成了仅需一个积分的问题，于是我们在优化ELOB的时候，只需分别优化\(q_{i}\)就可以了。将平均场假设代进ELOB中，化简可以得到 \[ \begin{aligned} ELOB &amp; =\int _{z_{j}} q_{j}( z_{j})\left[\underset{z_{i\neq j}}{\int \dotsc \int } q( z)\log p( x,z) dz_{i}\right] dz_{j} -\sum _{i}\int _{z_{i}} q_{i}( z_{i})\log q_{i}( z_{i}) dz_{i}\\ &amp; =\int _{z_{j}} q_{j}( z_{j}) E_{i\neq j}[\log p( x,z)] dz_{j} -\int _{z_{j}} q_{j}( z_{j})\log q_{j}( z_{j}) dz_{j} -\underbrace{\sum _{i\neq j}\int _{z_{i}} q_{i}( z_{i})\log q_{i}( z_{i}) dz_{i}}_{Const\ for\ j}\\ &amp; =\int _{z_{j}} q_{j}( z_{j})\log\frac{E_{i\neq j}[\log p( x,z)]}{q_{j}( z_{j})} dz_{j} -\underbrace{\sum ^{M}_{i\neq j}\int _{z_{i}} q_{i}( z_{i})\log q_{i}( z_{i}) dz_{i}}_{Const\ for\ j}\\ &amp; =-KL( E_{i\neq j}[\log p( x,z) ||q_{j}( z_{j})]) +const \end{aligned} \] 因为每个\(z_{j}\)都是相互独立，于是，只需分别最大化每个\(z_{j}\)的ELOB就可以实现ELOB最大化，而其他的项都视作了常数，此时，ELOB就简单地变成了一个负的KL距离，所以，想要最大化这个ELOB，我们只需要令 \[ q_{j}( z_{j}) =E_{i\neq j}[\log p( x,z)] \] 就可以了。这实际上是一个迭代的问题，因为在constant中，包含了其他的项的q，所以，我们只需不断地更新各个元素q的分布直到收敛就可以了。 从变分推断到VAE 但是，如果即使用了平均场假设也没法算，而使用MCMC又太慢怎么办？为了解决这个问题，我们回到最初的那个下界的表达式中 \[ \begin{aligned} \log p( x |\theta ) &amp; =\underbrace{E_{z\sim q( z)}(\log p( x ,z)) -H( q)}_{ELOB} +KL( q( z) ||p( z |x)) \end{aligned} \] 实际上ELOB有几种不同的，但是等价的表达方式： KL form : \[\mathcal{L}( \theta ;x) =E_{z\sim q( z)}(\log p_{\theta }( x|z)) -KL( q( z) ||p_{\theta }( z))\] Entropy form: \[\mathcal{L}( \theta ;x) =E_{z\sim q( z)}(\log p_{\theta }( x ,z)) -H( q)\] Fully Monte Carlo(FMC) form: \[\mathcal{L}( \theta ;x) =E_{z\sim q( z)}[\log p_{\theta }( x,z) -\log q( z)]\] 其中q是一个任意的分布，那么现在，我们令\(q( z) \triangleq q_{\phi }( z|x)\)，用KL形式的下界可以得到： \[ \mathcal{L}( \theta ,\phi ;x) =E_{z\sim q_{\phi }( z|x)}(\log p_{\theta }( x|z)) -KL( q_{\phi }( z|x) ||p_{\theta }( z)) \] 现在引入了一个带参数的\(q_{\phi }\)来表示这个上界，如果要最大化这个上界，我们只要用梯度上升不断更新参数$$就可以了。一般情况下，KL距离的那一项是有解析解的，所以梯度很好求。然而对第一项求梯度则没那么简单，一个常用的方法是 \[ \nabla _{\phi } E_{z\sim q_{\phi }( z)}( f( z)) =E_{z\sim q_{\phi }( z)}[ f( z) \nabla _{\phi }\log q_{\phi }( z)] \simeq \frac{1}{L}\sum ^{L}_{l=1} f\left( z^{l}\right) \nabla _{\phi }\log q_{\phi }\left( z^{l}\right) \] 但是这么做的方差太高，又因为\(q_{\phi }( z|x)​\)出现在了期望的采样分布中，没办法求梯度。 reparameterization 如上图，因为z是随机结点，所以，bp没法更新，我们可以用reparameterize trick来解决这个问题，这时z对于x来说就是一个固定的值，只要我们从\(\epsilon\)中抽样后，固定住就可以了，设 \[ z=g_{\phi }( \epsilon ,x) ,\epsilon \sim p( \epsilon ) \] 其中$$是一个已知的简单分布，比如说标准正态分布，次数z的产生就变成了从某个固定的标准分布中采样，于是下界中的期望那一项可以改写成： \[ E_{z\sim q_{\phi }( z|x)}(\log p_{\theta }( x|z)) =E_{\epsilon \sim p( \epsilon )}(\log p_{\theta }( x|g_{\phi }( \epsilon ,x))) \simeq \frac{1}{L}\sum ^{L}_{l=1}\log p_{\theta }( x|g_{\phi }( \epsilon ,x)) \] 于是对于一个样本\(x^{( i)}\)的下界可以写作： \[ \mathcal{L}\left( \theta ,\phi ;x^{( i)}\right) =\frac{1}{L}\sum ^{L}_{l=1}\log p_{\theta }\left( x^{( i)} |z^{( i,l)}\right) -KL\left( q_{\phi }\left( z^{( i)} |x^{( i)}\right) ||p_{\theta }\left( z^{( i)}\right)\right) \] 其中\(z^{( i,l)} =g_{\phi }\left( \epsilon ^{( i,l)} ,x^{( i)}\right) ,\epsilon ^{( l)} \sim p( \epsilon )\) 在这里，如果我们用一个MLP来表示\(p_{\theta}\)和\(q_{\phi}\)和就可以对用这个目标函数求梯度来最大化了，注意产生z的分布\(q_{\phi }\)其实是由一个标准正态分布的\(\epsilon\)和一个用MLP表示的映射函数\(g_{\phi}\)构成的，所以训练过程实际上是更新\(p_{\theta}\)和\(g_{\phi}\)这两个MLP的参数，我们称\(p_\theta\)为encoder network,\(q_{\phi}\)为decoder network。而z的产生则是从\(p( \epsilon )\)抽一个样本，然后经过一个确定性\(g_{\phi }\)来产生。 更直观一点，如果我们假设先验分布\(p( z)\),\(p( \epsilon )\)服从标准正态分布， \[ z=q_{\phi }( z|x) =g_{\phi }( \epsilon ,x) =\mu _{\phi }( x) +\Sigma _{\phi }^{1/2}( x) \epsilon \] 也就是说，\(q_{\phi }( z|x) \sim N\left( \mu _{\phi }( x) ,\Sigma _{\phi }^{1/2}( x)\right)\)也是正态分布，不过其参数由x决定。于是对于两个正态分布的KL距离，对于有J个维度的z，我们完全可以算出其解析解： \[ \begin{aligned} -KL( q_{\phi }(\mathbf{z} |\mathbf{x}) ||p_{\theta }(\mathbf{z})) &amp; =-KL( N(\mathbf{\mu }_{\phi } ,\mathbf{\sigma }_{\phi }) ||N(\mathbf{0} ,\mathbf{I}))\\ &amp; =\frac{1}{2}\sum ^{J}_{j=1}\left(\left( 1+\log \sigma ^{2}_{j}\right) -\mu ^{2}_{j} -\sigma ^{2}_{j}\right) \end{aligned} \] 接下来我们看看这个网络的架构 vae encoder network将一只喵星人映射成一个均值和一个方差，然后产生一个z样本，通过decoder network再变成一只喵~ 然而VAE对比GAN确实存在一些问题。 vse vs gan 可以看到VAE的“拟合”能力没有GAN的强，VAE会趋于平滑而GAN则不会。而且VAE产生的图像会比较模糊，这似乎所有优化对数似然的目标函数都有这问题(《Deep learning》) 参考资料 Auto-encoding variational bayes Tutorial on variational autoencoders How does the reparameterization trick for VAEs work and why is it important? Variational Autoencoders Explained Deep learning 徐亦达机器学习课程 带你理解EM算法]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正向跟反向KL距离到底有什么区别？]]></title>
    <url>%2F2018%2F03%2F06%2FKL-divergence%2F</url>
    <content type="text"><![CDATA[如果我们要用一个分布q去近似一个分布p，我们一般的做法都是去优化一个KL距离使得他最小，但是KL距离是一个不对称的距离，那么我们优化\(KL(q||p)\)跟\(KL(p||q)\)的区别在哪里? 首先考虑第一种KL距离\(KL(p||q)\)，也被称为M-projection 或 moment projection，定义如下： \[ KL(p||q)=\sum_xp(x)\ln\frac{p(x)}{q(x)} \] 在这个距离里面，我们发现只有当\(p(x)=0\)的时候，\(q(x)\)才能等于0，否则他们之间的距离就会无穷大，于是为了近似p，q会尽可能保持大于0，因为他能够等于0的地方太少了。于是在这种情况下q就会高估p的值域。 对于另外一个距离\(KL(q||p)\),又称为I-projection 或 information projection. \[ KL(q||p)=\sum_xq(x)\ln\frac{q(x)}{p(x)} \] 在这个距离里面，我们发现当\(p(x)=0\)的时候 ，我们必须要保证\(q(x)=0\)，否则这个距离就会变成无穷大，那我们的q就无法近似p了。所以，q很可能为了近似p，而避开或被p(x)=0的点截断。这种特性会导致q会低估p的值域。 图也正好表达了这种关系，\(KL(p||q)\)会高估p的值域尽可能保持大于0，而\(KL(q||p)\)会低估p的值域，被p的0点截断。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带你理解EM算法]]></title>
    <url>%2F2018%2F03%2F01%2FEM%2F</url>
    <content type="text"><![CDATA[很多时候，我们都要最大化似然度来求得一个参数$$的最优值。但是，很多时候，当我们的模型中存在隐变量的时候（比如，一个词所属的主题，聚类问题中样本的类别, etc.），我们的似然度是很难求的。下面是该似然度的式子，其中z表示不可观测的变量，x表示可观测的变量，由于z是不可观测的，所以，要求似然度，我们必须要对z求和或求积分(连续的时候求积分，离散的时候求和)。 \[ \mathcal{L}( \theta ) =\sum ^{N}_{i=1}\log p( x_{i} |\theta ) =\sum ^{N}_{i=1}\log\left[\sum _{z_{i}} p( x_{i} ,z_{i} |\theta )\right] \] 可以看到上面的这个式子，如果不存在隐变量的话，那么那个log是直接作用与p的，如果p恰好是指数族分布，那么这个似然度就非常好求，但是有隐变量的时候，log被一个\(\sum _{z}\)给截断的，这就使得这个式子变得很难优化。 这个问题的关键在于，\(\log p( x_{i} |\theta )\)很难优化，但是\(p( x_{i} ,z_{i} |\theta )\)却很好优化,比如说聚类的时候，你提前知道所有样本的类别了，那你计算每个类别的中心距离就太简单了，但是要优化\(p( x_{i} ,z_{i} |\theta )\)的前提是，你要看得到隐变量的取值才行啊，然而隐变量是看不到的。EM算法通过一个巧妙的构造，让\(p( x_{i} ,z_{i} |\theta )\)和似然度\(p( x_{i} |\theta )\)的下界联系起来，这是我们只要优化下界就能代替优化似然度本身。 接下来我们看一下对于单个样本\(p( x_{i})\)似然度的下界是什么东西。在这里我们引入了\(z_{i}\)的分布\(q_{i}( z_{i})\) \[ \begin{aligned} \log p( x_{i} |\theta ) &amp; =\log p( x_{i} ,z_{i}) -\log p( z_{i} |x_{i})\\ &amp; =\log\left(\frac{p( x_{i} ,z_{i})}{q_{i}( z_{i})}\right) -\log\left(\frac{p( z_{i} |x_{i})}{q_{i}( z_{i})}\right)\\ &amp; =\log p( x_{i} ,z_{i}) -\log q_{i}( z_{i}) -\log\left(\frac{p( z_{i} |x_{i})}{q_{i}( z_{i})}\right)\\ &amp; =\int q_{i}( z_{i})\log p( x_{i} ,z_{i}) dz-\int q_{i}( z_{i})\log q( z_{i}) dz-\int q_{i}( z_{i})\log\left(\frac{p( z_{i} |x_{i})}{q_{i}( z_{i})}\right) dz( 两边同时对z求期望)\\ &amp; =\underbrace{E_{z_{i}}(\log p( x_{i} ,z_{i})) -H( q_{i})}_{ELOB_{i}} +KL( q_{i}( z_{i}) ||p( z_{i} |x_{i})) \end{aligned} \] 我们知道\(KL( q( z_{i}) ||p( z_{i} |x_{i})) \geqslant 0\)，所以这个似然度一定有 \[ \log p( x_{i}) \geqslant E_{z_{i}}(\log p( x_{i} ,z_{i})) -H( q_{i}) \] 可以看到对数似然度被分解成了两部分，一个是evidence lower bound(ELOB)，似然度的下界，另一个是KL距离，不管q是什么分布，这两部分加起来肯定是一样的。 ELOB与KL 图中的L是我们的ELOB。 也就是说，只要我们令KL距离为0，此时 \(q(z) =p(z|x)\) ，那么ELOB就等于似然度的值了。这就意味着我们最大化 \(\theta\) 的时候，不再需要对\(\log p(x|\theta)\)做，只要找到 \(\theta\) 使得这个ELOB最大不就相当于在“最大化我们的似然度”吗。而最大化这个ELOB太简单了，在这里\(H(q)\) 是q的熵，与 \(\theta\) 无关只与分布q有关，所以不用管。于是我们把 \(q(z) =p(z|x)\) 代入到ELOB中得到 \[ \begin{aligned} ELOB_{i} &amp; =E_{z_{i}}(\log p( x_{i} ,z_{i})) +const\\ &amp; =\int q_{i}( z_{i})\log p( x_{i} ,z_{i}) dz+const\\ &amp; =\int p( z_{i} |x_{i})\log p( x_{i} ,z_{i}) dz+const\\ &amp; =\sum _{z_{i}} p( z_{i} |x_{i})\log p( x_{i} ,z_{i}) +const( 如果z是离散的) \end{aligned} \] EM算法示意图 EM算法，示意图，E步，把KL设为0，蓝色的线往上移，使得ELOB=似然度，M步，最大化ELOB，使得似然度增大，红色的线往上移，然后我们不断重复直到收敛。 考虑所有样本，正式的EM框架： E步：把\(q_{i}( z_{i}) =p( z_{i} |x_{i})\)代入到下界中，再把常数项剔除， \[ Q\left( \theta ,\theta ^{t-1}\right) =\sum ^{N}_{i=1}\sum _{z_{i}} p( z_{i} |x_{i},\theta^{t-1})\log p( x_{i} ,z_{i},\theta)=\sum ^{N}_{i=1} E\left[\log p( x_{i} ,z_{i} |\theta ) |x_{i} ,\theta ^{t-1}\right] \] M步：最大化下界ELOB \[ \theta ^{t} =\arg\max_{\theta } Q\left( \theta ,\theta ^{t-1}\right) \] M步2：我们还可以做MAP估计，只需要在Q加上参数的对数先验就可以轻松完成，E步没有任何变化 \[ \theta ^{t} =\arg\max_{\theta } Q\left( \theta ,\theta ^{t-1}\right) +\log p( \theta ) \] 在MAP估计的时候，不仅需要考虑下界的最大化，还需要考虑先验对参数的影响。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社交网络中的Link Prediction]]></title>
    <url>%2F2018%2F02%2F26%2FLinkPrediction%2F</url>
    <content type="text"><![CDATA[介绍 给你一段时间内的社交网络关系，我们能否预测出成员之间在未来的互动？ 我们称该问题为Link prediction Problem.他还有另外一些常见的应用，比如社交网站的好友推荐，预测蛋白质间的相互影响，预测犯罪嫌疑人的关系，商品推荐等等。 在基于社交网络的Link predition问题中，我们通常要考虑社交网络所带有的特有的特质。所以我们先介绍一下社交网络的几个有趣的性质： power law degree distribution the small world phenomenon the community structure (clustering effect) etc Power law degree distribution: 大部分人都只有很少的链接，但是有一小部分人，他们的链接的数量远远多于其他人。 power law degree distribution Small-World Phenomenon: 或者叫六度空间，你和任何一个陌生人之间所间隔的人不会超过六个 community structure (clustering effect): 社交网络里面会有很多个小群体，他们都相互认识彼此。 那么到底要怎么去做link predction呢，目前传统的方法，有Path-based Methods,Neighbor-based Methods等等。下面是他们的介绍。 Path-based Methods Graph Distance 一个最直接的预测方法就是计算两个结点间的距离，然后根据距离的大小来预测，两个结点越近那么就越容易在未来建立联系。但是在上百万的结点下直接用dijkstra算法是非常低效的。相反，我们可以利用small world phenomenon来提高我们的效率。 比如说要计算x，y两点距离，我们先初始化两个集合 \(S=\{x\},D=\{y\}\),然后开始S和D的集合，扩展的方法就是不断地把集合里面元素的邻居放进去，直到S和D相同的元素为止。根据small world phenomenon来说，扩展的次数不会太多。另外效率起见，我们一般选择元素数量较少的那个来扩展。 Katz (Exponentially Damped Path Counts) 我们还可以考虑用x，y之间存在的路径的数量来衡量它们的距离。然而，路径有长有短，一般认为，那些很长的路径其实是没什么说服力的，于是引入指数衰减机制随着路径长度进行衰减。 \[ Score(x,y)=\sum_{l=1}^{\infty}\beta^l|path_{x,y}^l| \] \(\beta\)就是指数衰减的系数，\(path^l\)表示那些长度为l的路径。 Hitting Time 为了加快计算速度，可以使用蒙特卡洛的技术来估计x，y的路径的数量。从x出发，在附近随机的跳转，如果到达y，则记录下这次到达y的所需跳转次数。最后我们用 总跳转次数/到达y的次数 来表示距离。 \[ Score(x,y)=-H_{x,y} \] 其中\(H_{x,y}\)为总跳转次数/到达y的次数，我们取负H来表示评分,H越小表示越近则越好。 Rooted PageRank 然而，如果y是一个非常有影响力的人，那么很多人都能在非常少的跳转次数下到达y，为了减轻这效应，我们增加一个随机“reset”以及继续游走的机制。当到达y时，以概率\(\alpha\)跳回x，以\(1-\alpha\)继续随机游走。并记录下经过y的次数。 \[ Score(x,y)=-H_{x,y}\pi_y \] 其中\(\pi_y\)表示那么多次跳转经过y概率。 Neighbor-based Methods Common Neighbors 当两个用户有着很多个相同的邻居，我们就认为这两个用户很有可能建立联系。所以两个用户的相似性就用他们相同邻居的数量表示： \[ Score(x,y)=|\mathcal{T}(x)\cap\mathcal{T}(y)| \] 其中\(\mathcal{T}(x)\)表示x的邻居。实际上这个方法揭示了一个叫“closing a trangle”的现象 Jaccard’s Coefficient 然而Common Neighbors有一个很大的问题，假设有一个人有非常多的邻居，那么所有人都会倾向于预测跟他产生互动，为此，我们还要把他们邻居的数量考虑进去，于是我们认为，如果两个人共同邻居的数量在他们所有好友数量中占比越大，就认为可能建立联系。即 \[ Score(x,y)=\frac{|\mathcal{T}(x)\cap\mathcal{T}(y)|}{|\mathcal{T}(x)\cup\mathcal{T}(y)|} \] Adamic/Adar (Frequency-Weighted Common Neighbors) 这个方法同样是对Common Neighbors的改进，当我们计算两个相同邻居的数量的时候，其实每个邻居的“重要程度”都是不一样的，我们认为这个邻居的邻居数量越少，就越凸显它作为“中间人”的重要性，毕竟一共只认识那么少人，却恰好是x，y的好朋友。 \[ Score(x,y)=\sum_{Z\in \mathcal{T}(x)\cap\mathcal{T}(y)}\frac{1}{\log |\mathcal{T}(z)|} \] Friendes-mearsure 既然两个人有相同的好友可以表达他们间的距离，那么我们可以把这一个思想推广，我们认为，他们的好友之间很有可能互为好友。我们就计算他们好友之间互为好友的数量作为评价标准。 friends-measure Preferential Attachment 另外，如果两个用户拥有的好友数量越多，那么就越有可能更愿意去建立联系。也就是“富人越富”原则，基于这思想,用他们两个用户的好友数量的乘积作为评分。 \[ Score(x,y)=|\mathcal{T}(x)||\mathcal{T}(y)| \] Link Prediction with Personalized Social Influence 上面的方法只考虑了结构，现在介绍一种考虑了用户行为的方法，比如转发，评论，点赞等。 这里考虑一种低秩表达,S和T，使得未来会建立联系的用户i和用户j有, \[ S_iT_j&gt;S_iT_{n} \] 其中(i,n)是那些不会建立联系的人。 用一个sigmoid函数来表达下一时刻会active的概率。这里的f使用了log函数，主要考虑了其影响是随着次数指数衰减的。 最后给出一个目标函数通过优化得到S和T。 Link Prediction via Subgraph Embedding-Based Convex Matrix Completion 还有另外一个方法，它考虑了subgraph. 基本思想就是，每个结点，用广度优先搜索就可以得到不同深度的子图，然后利用这些结构信息来embedding,最后通过将不同深度得到的embedding concat在一起，就得到了这个结点的embedding. 最后就用这些embedding的余弦相似度来做link prediction. References What will Facebook friendships look like tomorrow? Link Prediction via Subgraph Embedding-Based Convex Matrix Completion. Zhu Cao, Linlin Wang, Gerard De melo.AAAI 2018. Link Prediction with Personalized Social Influence. Huo, Zepeng, Xiao Huang, Xia Hu. AAAI 2018。 Xuezhi Cao, Haokun Chen, Xuejian Wang, Weinan Zhang, Yong Yu. The Thirty-Second AAAI Conference on Artificial Intelligence AAAI 2018.]]></content>
      <tags>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PageRank算法在社交网络上的应用]]></title>
    <url>%2F2018%2F02%2F19%2FPageRank%2F</url>
    <content type="text"><![CDATA[PageRank算法介绍 pagerank算法的核心思想是，计算一个用户随机点击一个网站然后不停点击从而到达各个网站的概率。而一个网站的打开概率又取决于那些指向他自己的那些网站的概率，所以这个概率的计算是一个不断迭代的过程。 一个简单的例子：B,C,D同时指向A，我们认为，BCD的PR是0.25，那么A的PR值就是0.75 1519042867642 但是，如下图，如果网站D有3个外链，那么你从网站D跳到网站A的概率就不一定是100%了，这是我们要给它做一个权重衰减，我们给PR值除以3 1519048276921 这个模型可以写作以下公式： \[ PR(u) = \sum_{v \in B_u} \frac{PR(v)}{L(v)} \] 其中L表示结点的出度，\(B_u\)是所有指向u的结点。然而一个用户在点击网页的时候是不会无限点下去了，他最终肯定会在某个结点上停止，于是，我们可以引入一个damping factor来表达这种关系，当你计算PR的时候，要乘一个衰减的系数来认为有一定概率会在上一个页面停止，而不会跳转到这个页面来。于是PR的公式可以改写成这样： \[ PR(p_i) = \frac{1-d}{N} + d \sum_{p_j \in M(p_i)} \frac{PR (p_j)}{L(p_j)} \] d就是damping factor,d一般取0.85,N是结点数量，那个1-d/N是为了保证这个概率值在0到1之间。这个表达式可以写成矩阵的形式： \[ \mathbf{R} = \begin{bmatrix} PR(p_1) \\ PR(p_2) \\ \vdots \\ PR(p_N) \end{bmatrix} \] \[ \mathbf{R} = \begin{bmatrix} {(1-d)/ N} \\ {(1-d) / N} \\ \vdots \\ {(1-d) / N} \end{bmatrix} + d \begin{bmatrix} \ell(p_1,p_1) &amp; \ell(p_1,p_2) &amp; \cdots &amp; \ell(p_1,p_N) \\ \ell(p_2,p_1) &amp; \ddots &amp; &amp; \vdots \\ \vdots &amp; &amp; \ell(p_i,p_j) &amp; \\ \ell(p_N,p_1) &amp; \cdots &amp; &amp; \ell(p_N,p_N) \end{bmatrix} \mathbf{R} \] 其中\(l(p_i,p_j)\)表示结点\(p_i\)对\(p_j\)的影响程度，比如在例子2，里面，\(l(B,A)=1/2\).写成矩阵形式,这里P其实相当于邻接矩阵： \[ \mathbf{R} = d P\mathbf{R} + \frac{1-d}{N} \mathbf{1} \] 我们只要求解这个R，就能得到每个结点的PR值。 Ranking Users in Social Networks with Higher-Order Structures 这里介绍一种改进的方法，这是在社交网络上的应用，在计算PR的时候，其实我们默认了，在一个网站上以相同概率跳转到其他的结点，但这其实在社交网络里面是有问题的。看下面的例子。 用户1同时关注了2,3,4在三个用户，但是，很显然，用户1其实是更信任用户2多过用户4的，因为用户1同时关注了2跟3. 1519050454857 所以我们要做的就是，考虑这种三角结构： 1519052828959 一共有7种。举个例子，当我们考虑M6时。 1519052801833 对于结点3而言，M6结构一共出现了2次，分别是153,123.所以矩阵第1行第3列等于2. 上面的这个考虑了三角结构的邻接矩阵可以用下面的公式计算。其中\(B=W\odot W^T\),\(U=W-B\),其中\(\odot\)是对应元素相乘 1519053031829 最后对于PR的计算公式： \[ \mathbf{R} = d P\mathbf{R} + \frac{1-d}{N} \mathbf{1} \] 我们用 \[ H_{M_k}=\alpha W+(1-\alpha)W_{M_k} \] 来替换掉P就能取得很好的效果。 扩展资料 其实PR只是目前页面排序的一个小小的权重，这是目前谷歌最新的企鹅算法 参考资料 Zhao, Huan, et al. “Ranking Users in Social Networks with Higher-Order Structures.” (AAAI 2018) PageRank-wiki]]></content>
      <tags>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computational Learning Theory and Model Selection]]></title>
    <url>%2F2018%2F02%2F18%2FComputational%20Learning%20Theory%20and%20Model%20Selection%2F</url>
    <content type="text"><![CDATA[True vs. Empirical Risk 当你有一个函数f的时候，你自然就想知道它到底是好还是坏，那么怎么评价呢？我们可以用一个叫risk的东西来衡量这个函数的风险，这里的risk可以直观地看做你函数的错误率。然而，根据no free lunch定理，显然没有一个函数永远是最优的，那这个risk到底是什么？它其实是在你数据分布\(\mathcal{D}\)上的风险。而我们通常用empirical risk来对true risk进行估计，也就是对错误率估计,更一般empirical risk可以称为经验误差，当你的样本量越多的时候，你对函数f的风险的估计就越准确。 True Risk: Classification - 误分类概率 \(P(f(X)\ne Y)\) Regression - Mean Squared Error \(\mathbb{E}[(f(X)-Y)^2]\) Empirical Risk: Classification - \(\frac{1}{n}\sum_{i=1}^n1_{f(X_i\ne Y_i)}\) Regression - \(\frac{1}{n}\sum_{i=1}^n{(f(X_i)- Y_i)^2}\) %可是我们仅仅用经验误差是不够的，因为经验误差只考虑了“训练集”的误差，我们可以很容易构造出一个函数使得他完美拟合所有数据。所以我们还要考虑泛化误差才行。 我们设存在一个完美的risk,\(R^*\) ,还有一个从n个数据中估计出来的f以及它对应的risk，\(E[R(\hat{f_n})]\)，我们定义一个Excess risk为\(E[R(\hat{f_n})]-R^*\),于是这个excess risk可以分解为两部分，分别是estimation error 和 approximation error. 1518535927188 直观来看，estimation error，是因为缺少足够的样本，从而导致我们从函数族\(\mathcal{F}\)选择模型时没法取得最优的模型而产生的误差。而approximation error是由于函数族\(\mathcal{F}\)的限制而产生的误差，比如说线性回归，由于限制在了线性的空间中，而对非线性的数据存在误差。 简单地说，estimation error是样本的问题，approximation error是模型复杂度的问题。 然而模型越复杂，所需的样本也就越多，这就形成一个平衡，你需要一个合适的hypothese class \(\mathcal{F}\)大小。 1518535937954 Learning Theory Empirical Risk Minimization 1518537111943 三个重要的引理 1518537123984 1518881523085 使用这几个引理可以让我们证明在learning theory中非常重要的结论。第三个定理是，一般套路就是考虑只有一个样本改变后两个相减的差，小于某个d，然后就可以套这个不等式，然后令最右边的是\(\delta\)，就得出了一个界。 现在我们来定义一下Hypothesis Class，数据集不同的划分方式其实就对应了不同的假设类，而所有的划分方法就组成了假设类\(\mathcal{H}\). 这个是跟，“概念类”对应的一个概念，所谓概念类其实就是数据正确的标签\(\mathcal{C}\).，所以我们要做的事情就是搜索出一个最优的假设h，使得逼近正确的标签。 Finite Hypothesis Space 对于有限的假设空间，有以下定理 1518881888335 上面这个引理是由hoeffding 不等式的出来的，因为这里的\(\hat{E}\)对应的是经验误差，是对E的近似，所以直接用hoeffding不等式。 1518877689847 令\(\delta=2|\mathcal{H}|\exp(-2m\epsilon^2)\)即可得12.19 上面定理用了引理1的集合的不等式。通过最下面的这个界可以看到，当\(|H|\)越大的时候这个界是越大的。 Infinite Hypothesis Space 当假设类是无限时，我们可以构造出一个叫增长函数的东西，用来表示假设空间H对m个样本所能赋予的最大可能结果数。 1518882559587 如果我们的假设空间能够对这所有m个样本赋予任意的标签，那么H在m个样本下是可以打散这个数据集D的，那么我们定义那个可以打散的m的最大值，称为H的VC维。也就是说，VC维是用于衡量假设空间\(|H|\)大小的一个东西。 1518882573822 这样我们就能用VC维来构造出经验误差的界,VC维有点像有限假设类下的类比。 1518882637789 Rademacher 复杂度 Rademacher 复杂度，他是对VC维的一个改进，因为VC维在刻画假设空间的大小时，并没有考虑数据的分布，这使得他的界非常松，实际意义比较小。 1518882752502 这里\(y_i\)取1，-1 ，然而这里yi其实是指现实中的值，而如果考虑更一般的情况，我们将y_i换成一个1，-1的随机变量，于是，问题转化为，我们希望找到一个h，使得这个最大（等价于经验误差最小） 1518882879727 这里我的理解是，因为sup是在\(\sigma_i\)外面的，所以当我们选择h的时候，\(\sigma_i\)相当于是固定的，也就是可以看做是一个随机标签的样本，于是如果我们总能找到一个h使得每个\(h(x_i)=\sigma_i\)，那么这个东西就等于1，那这个假设空间就很棒。 1518883423948 最后这里考虑的是在数据集D下m个样本的函数空间F的复杂度，它的做法就是对所有从D中采样，而且是m个样本的数据集Z进行积分，也就是考虑所有m个样本的组合，然后求均值，最后的就是要求的平均复杂度，我们可以通过这个函数空间的复杂度来给出这泛化误差的界。 Reference Computational Learning Theory What does the term “Estimation error” mean? Excess Error, Approximation Error, and Estimation Error 如何通俗的理解机器学习中的VC维、shatter和break point]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
