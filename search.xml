<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[社交网络中的Link Prediction]]></title>
    <url>%2F2018%2F02%2F21%2FLinkPrediction%2F</url>
    <content type="text"><![CDATA[介绍 给你一段时间内的社交网络关系，我们能否预测出成员之间在未来的互动？ 我们称该问题为Link prediction Problem.常见的应用就是社交网站的好友推荐，在信息生物中预测蛋白质间的相互影响，预测犯罪嫌疑人的关系，商品推荐等等。 首先介绍一下社交网络的几个有趣的性质： power law degree distribution the small world phenomenon the community structure (clustering effect) etc Power law degree distribution: 大部分人都只有很少的链接，但是有一小部分人，他们的链接的数量远远多于其他人。 power law degree distribution Small-World Phenomenon: 或者叫六度空间，你和任何一个陌生人之间所间隔的人不会超过六个 community structure (clustering effect): 社交网络里面会有很多个小群体，他们都相互认识彼此。 那么到底要怎么去做link predction呢，目前传统的方法，有Path-based Methods,Neighbor-based Methods等等。下面是他们的介绍。 Path-based Methods Graph Distance 一个最直接的预测方法就是计算两个结点间的距离，然后根据距离的大小来预测。但是在上百万的结点下直接用dijkstra算法是非常低效的。相反，我们可以利用small world phenomenon来提高我们的效率。 比如说要计算x，y两点距离，我们先初始化两个集合 \(S=\{x\},D=\{y\}\),然后开始S和D的集合，扩展的方法就是不断地把集合里面元素的邻居放进去，直到S和D相同的元素为止。根据small world phenomenon来说，扩展的次数不会太多。另外效率起见，我们一般选择元素数量较少的那个来扩展。 Katz (Exponentially Damped Path Counts) 我们还可以考虑用x，y之间存在的路径的数量来衡量它们的距离。然而，路径有长有短，一般认为，那些很长的路径其实是没什么说服力的，于是引入指数衰减机制随着路径长度进行衰减。 \[ Score(x,y)=\sum_{l=1}^{\infty}\beta^l|path_{x,y}^l| \] \(\beta\)就是指数衰减的系数，\(path^l\)表示那些长度为l的路径。 Hitting Time 为了加快计算速度，可以使用蒙特卡洛的技术来估计x，y的路径的数量。从x出发，在附近随机的跳转，如果到达y，则记录下这次到达y的所需跳转次数。最后我们用 总跳转次数/到达y的次数 来表示距离。 \[ Score(x,y)=-H_{x,y} \] 其中\(H_{x,y}\)为总跳转次数/到达y的次数，我们取负H来表示评分,H越小表示越近则越好。 Rooted PageRank 然而，如果y是一个非常有影响力的人，那么很多人都能在非常少的跳转次数下到达y，为了减轻这效应，我们增加一个随机&quot;reset&quot;的机制以及不停。当到达y时，以概率跳回x，以\(1-\alpha\)继续随机游走。并记录下经过y的次数。 \[ Score(x,y)=-H_{x,y}\pi_y \] 其中\(\pi_y\)表示那么多次跳转经过y概率。 Neighbor-based Methods Common Neighbors 当两个用户有着很多个相同的邻居，我们就认为这两个用户很有可能建立联系。所以两个用户的相似性就用 \[ Score(x,y)=|\mathcal{T}(x)\cap\mathcal{T}(y)| \] 其中\(\mathcal{T}(x)\)表示x的邻居。实际上这个方法揭示了一个叫“closing a trangle”的现象 Jaccard’s Coefficient 然而Common Neighbors有一个很大的问题，假设有一个人有非常多的邻居，那么所有人都会被预测成会更他互动了，所以，我们还要把他们邻居的数量考虑进去，于是我们认为，如果两个人共同邻居的数量在他们所有好友数量中占比越大，就认为可能建立联系。即 \[ Score(x,y)=\frac{|\mathcal{T}(x)\cap\mathcal{T}(y)|}{|\mathcal{T}(x)\cup\mathcal{T}(y)|} \] Adamic/Adar (Frequency-Weighted Common Neighbors) 这个方法同样是对Common Neighbors的改进，当我们计算两个相同邻居的数量的时候，其实每个邻居的“重要程度”都是不一样的，我们认为这个邻居的邻居数量越少，就越凸显它作为“中间人”的重要性，毕竟一共只认识那么少人，却恰好是x，y的好朋友。 \[ Score(x,y)=\sum_{Z\in \mathcal{T}(x)\cap\mathcal{T}(y)}\frac{1}{\log |\mathcal{T}(z)|} \] Friendes-mearsure 既然两个人有相同的好友可以表达他们间的距离，那么我们可以把这一个思想推广，我们认为，他们的好友之间很有可能互为好友。我们就计算他们好友之间互为好友的数量作为评价标准。 friends-measure Preferential Attachment 另外，如果两个用户拥有的好友数量越多，那么就越有可能更愿意去建立联系。也就是“富人越富”原则，基于这思想,用他们两个用户的好友数量的乘积作为评分。 \[ Score(x,y)=|\mathcal{T}(x)||\mathcal{T}(y)| \] Link Prediction with Personalized Social Influence 上面的方法只考虑了结构，现在介绍一种考虑了用户行为的方法，比如转发，评论，点赞等。 这里考虑一种低秩表达,S和T，使得未来会建立联系的用户i和用户j有, \[ S_iT_j&gt;S_iT_{n} \] 其中(i,n)是那些不会建立联系的人。 用一个sigmoid函数来表达下一时刻会active的概率。这里的f使用了log函数，主要考虑了其影响是随着次数指数衰减的。 最后给出一个目标函数通过优化得到S和T。 Link Prediction via Subgraph Embedding-Based Convex Matrix Completion 还有另外一个方法，它考虑了subgraph. 基本思想就是，每个结点，用广度优先搜索就可以得到不同深度的子图，然后利用这些结构信息来embedding,最后通过将不同深度得到的embedding concat在一起，就得到了这个结点的embedding. 最后就用这些embedding的余弦相似度来做link prediction. References What will Facebook friendships look like tomorrow? Link Prediction via Subgraph Embedding-Based Convex Matrix Completion. Zhu Cao, Linlin Wang, Gerard De melo.AAAI 2018. Link Prediction with Personalized Social Influence. Huo, Zepeng, Xiao Huang, Xia Hu. AAAI 2018。]]></content>
      <tags>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction of PageRank and Application in Social Network with Higher-Order Structures]]></title>
    <url>%2F2018%2F02%2F19%2FPageRank%2F</url>
    <content type="text"><![CDATA[PageRank算法介绍 pagerank算法的核心思想是，计算一个用户随机点击一个网站然后不停点击从而到达各个网站的概率。而一个网站的打开概率又取决于那些指向他自己的那些网站的概率，所以这个概率的计算是一个不断迭代的过程。 一个简单的例子：B,C,D同时指向A，我们认为，BCD的PR是0.25，那么A的PR值就是0.75 1519042867642 但是，如下图，如果网站D有3个外链，那么你从网站D跳到网站A的概率就不一定是100%了，这是我们要给它做一个权重衰减，我们给PR值除以3 1519048276921 这个模型可以写作以下公式： \[ PR(u) = \sum_{v \in B_u} \frac{PR(v)}{L(v)} \] 其中L表示结点的出度，\(B_u\)是所有指向u的结点。然而一个用户在点击网页的时候是不会无限点下去了，他最终肯定会在某个结点上停止，于是，我们可以引入一个damping factor来表达这种关系，当你计算PR的时候，要乘一个衰减的系数来认为有一定概率会在上一个页面停止，而不会跳转到这个页面来。于是PR的公式可以改写成这样： \[ PR(p_i) = \frac{1-d}{N} + d \sum_{p_j \in M(p_i)} \frac{PR (p_j)}{L(p_j)} \] d就是damping factor,d一般取0.85,N是结点数量，那个1-d/N是为了保证这个概率值在0到1之间。这个表达式可以写成矩阵的形式： \[ \mathbf{R} = \begin{bmatrix} PR(p_1) \\ PR(p_2) \\ \vdots \\ PR(p_N) \end{bmatrix} \] \[ \mathbf{R} = \begin{bmatrix} {(1-d)/ N} \\ {(1-d) / N} \\ \vdots \\ {(1-d) / N} \end{bmatrix} + d \begin{bmatrix} \ell(p_1,p_1) &amp; \ell(p_1,p_2) &amp; \cdots &amp; \ell(p_1,p_N) \\ \ell(p_2,p_1) &amp; \ddots &amp; &amp; \vdots \\ \vdots &amp; &amp; \ell(p_i,p_j) &amp; \\ \ell(p_N,p_1) &amp; \cdots &amp; &amp; \ell(p_N,p_N) \end{bmatrix} \mathbf{R} \] 其中\(l(p_i,p_j)\)表示结点\(p_i\)对\(p_j\)的影响程度，比如在例子2，里面，\(l(B,A)=1/2\).写成矩阵形式,这里P其实相当于邻接矩阵： \[ \mathbf{R} = d P\mathbf{R} + \frac{1-d}{N} \mathbf{1} \] 我们只要求解这个R，就能得到每个结点的PR值。 Ranking Users in Social Networks with Higher-Order Structures 这里介绍一种改进的方法，这是在社交网络上的应用，在计算PR的时候，其实我们默认了，在一个网站上以相同概率跳转到其他的结点，但这其实在社交网络里面是有问题的。看下面的例子。 用户1同时关注了2,3,4在三个用户，但是，很显然，用户1其实是更信任用户2多过用户4的，因为用户1同时关注了2跟3. 1519050454857 所以我们要做的就是，考虑这种三角结构： 1519052828959 一共有7种。举个例子，当我们考虑M6时。 1519052801833 对于结点3而言，M6结构一共出现了2次，分别是153,123.所以矩阵第1行第3列等于2. 上面的这个考虑了三角结构的邻接矩阵可以用下面的公式计算。其中\(B=W\odot W^T\),\(U=W-B\),其中\(\odot\)是对应元素相乘 1519053031829 最后对于PR的计算公式： \[ \mathbf{R} = d P\mathbf{R} + \frac{1-d}{N} \mathbf{1} \] 我们用 \[ H_{M_k}=\alpha W+(1-\alpha)W_{M_k} \] 来替换掉P就能取得很好的效果。 扩展资料 其实PR只是目前页面排序的一个小小的权重，这是目前谷歌最新的企鹅算法 参考资料 Zhao, Huan, et al. &quot;Ranking Users in Social Networks with Higher-Order Structures.&quot; (AAAI 2018) PageRank-wiki]]></content>
      <tags>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computational Learning Theory and Model Selection]]></title>
    <url>%2F2018%2F02%2F18%2FComputational%20Learning%20Theory%20and%20Model%20Selection%2F</url>
    <content type="text"><![CDATA[True vs. Empirical Risk 当你有一个函数f的时候，你自然就想知道它到底是好还是坏，那么怎么评价呢？我们可以用一个叫risk的东西来衡量这个函数的风险，这里的risk可以直观地看做你函数的错误率。然而，根据no free lunch定理，显然没有一个函数永远是最优的，那这个risk到底是什么？它其实是在你数据分布\(\mathcal{D}\)上的风险。而我们通常用empirical risk来对true risk进行估计，也就是对错误率估计,更一般empirical risk可以称为经验误差，当你的样本量越多的时候，你对函数f的风险的估计就越准确。 True Risk: Classification - 误分类概率 \(P(f(X)\ne Y)\) Regression - Mean Squared Error \(\mathbb{E}[(f(X)-Y)^2]\) Empirical Risk: Classification - \(\frac{1}{n}\sum_{i=1}^n1_{f(X_i\ne Y_i)}\) Regression - \(\frac{1}{n}\sum_{i=1}^n{(f(X_i)- Y_i)^2}\) %可是我们仅仅用经验误差是不够的，因为经验误差只考虑了“训练集”的误差，我们可以很容易构造出一个函数使得他完美拟合所有数据。所以我们还要考虑泛化误差才行。 我们设存在一个完美的risk,\(R^\*\) ,还有一个从n个数据中估计出来的f以及它对应的risk，\(E[R(\hat{f_n})]\)，我们定义一个Excess risk为\(E[R(\hat{f_n})]-R^*\),于是这个excess risk可以分解为两部分，分别是estimation error 和 approximation error. 1518535927188 直观来看，estimation error，是因为缺少足够的样本，从而导致我们从函数族\(\mathcal{F}\)选择模型时没法取得最优的模型而产生的误差。而approximation error是由于函数族\(\mathcal{F}\)的限制而产生的误差，比如说线性回归，由于限制在了线性的空间中，而对非线性的数据存在误差。 简单地说，estimation error是样本的问题，approximation error是模型复杂度的问题。 然而模型越复杂，所需的样本也就越多，这就形成一个平衡，你需要一个合适的hypothese class \(\mathcal{F}\)大小。 1518535937954 Learning Theory Empirical Risk Minimization 1518537111943 三个重要的引理 1518537123984 1518881523085 使用这几个引理可以让我们证明在learning theory中非常重要的结论。第三个定理是，一般套路就是考虑只有一个样本改变后两个相减的差，小于某个d，然后就可以套这个不等式，然后令最右边的是\(\delta\)，就得出了一个界。 现在我们来定义一下Hypothesis Class，数据集不同的划分方式其实就对应了不同的假设类，而所有的划分方法就组成了假设类\(\mathcal{H}\). 这个是跟，“概念类”对应的一个概念，所谓概念类其实就是数据正确的标签\(\mathcal{C}\).，所以我们要做的事情就是搜索出一个最优的假设h，使得逼近正确的标签。 Finite Hypothesis Space 对于有限的假设空间，有以下定理 1518881888335 上面这个引理是由hoeffding 不等式的出来的，因为这里的\(\hat{E}\)对应的是经验误差，是对E的近似，所以直接用hoeffding不等式。 1518877689847 令\(\delta=2|\mathcal{H}|\exp(-2m\epsilon^2)\)即可得12.19 上面定理用了引理1的集合的不等式。通过最下面的这个界可以看到，当\(|H|\)越大的时候这个界是越大的。 Infinite Hypothesis Space 当假设类是无限时，我们可以构造出一个叫增长函数的东西，用来表示假设空间H对m个样本所能赋予的最大可能结果数。 1518882559587 如果我们的假设空间能够对这所有m个样本赋予任意的标签，那么H在m个样本下是可以打散这个数据集D的，那么我们定义那个可以打散的m的最大值，称为H的VC维。也就是说，VC维是用于衡量假设空间\(|H|\)大小的一个东西。 1518882573822 这样我们就能用VC维来构造出经验误差的界,VC维有点像有限假设类下的类比。 1518882637789 Rademacher 复杂度 Rademacher 复杂度，他是对VC维的一个改进，因为VC维在刻画假设空间的大小时，并没有考虑数据的分布，这使得他的界非常松，实际意义比较小。 1518882752502 这里\(y_i\)取1，-1 ，然而这里yi其实是指现实中的值，而如果考虑更一般的情况，我们将y_i换成一个1，-1的随机变量，于是，问题转化为，我们希望找到一个h，使得这个最大（等价于经验误差最小） 1518882879727 这里我的理解是，因为sup是在\(\sigma_i\)外面的，所以当我们选择h的时候，\(\sigma_i\)相当于是固定的，也就是可以看做是一个随机标签的样本，于是如果我们总能找到一个h使得每个\(h(x_i)=\sigma_i\)，那么这个东西就等于1，那这个假设空间就很棒。 1518883423948 最后这里考虑的是在数据集D下m个样本的函数空间F的复杂度，它的做法就是对所有从D中采样，而且是m个样本的数据集Z进行积分，也就是考虑所有m个样本的组合，然后求均值，最后的就是要求的平均复杂度，我们可以通过这个函数空间的复杂度来给出这泛化误差的界。 Reference Computational Learning Theory What does the term “Estimation error” mean? Excess Error, Approximation Error, and Estimation Error 如何通俗的理解机器学习中的VC维、shatter和break point]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
